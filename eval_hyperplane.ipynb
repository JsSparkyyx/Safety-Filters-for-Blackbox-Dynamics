{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.Safe2Unsafe import DeepAccidentDataset\n",
    "from method.dynamics import HyperplaneEncoder\n",
    "from method.barriers import DiscriminatingHyperplane\n",
    "from method.trainers import HyperplaneTrainer\n",
    "import torch\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DeepAccidentDataset(train_batch_size=32,val_batch_size=32,num_workers=16)\n",
    "data.setup()\n",
    "train_dataloader = data.train_dataloader()\n",
    "test_dataloader = data.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HyperplaneTrainer(\n",
       "  (model): HyperplaneEncoder(\n",
       "    (encoder): ViTAttentionEncoder(\n",
       "      (ViT): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "            (position_embedding): Embedding(197, 768)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (1): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (2): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (3): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (4): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (5): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (6): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (7): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (8): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (9): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (10): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (11): CLIPEncoderLayer(\n",
       "                (self_attn): CLIPAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=774, out_features=16, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (3): ReLU()\n",
       "      )\n",
       "      (linear): Linear(in_features=34, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (barrier): DiscriminatingHyperplane(\n",
       "    (cbf): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 16\n",
    "barrier = DiscriminatingHyperplane(2,latent_dim=latent_dim)\n",
    "# model = HyperplaneEncoder(2,\"cuda\",model=\"google/vit-base-patch16-224\",latent_dim=latent_dim)\n",
    "model = HyperplaneEncoder(2,\"cuda\",model=\"openai/clip-vit-base-patch16\",latent_dim=latent_dim)\n",
    "# model = HyperplaneEncoder(2,\"cuda\",model=\"resnet\",latent_dim=latent_dim)\n",
    "trainer = HyperplaneTrainer(model,barrier)\n",
    "checkpoint = torch.load(\"/root/tf-logs/DiscriminatingHyperplane/version_1/checkpoints/last.ckpt\")\n",
    "trainer.load_state_dict(checkpoint['state_dict'])\n",
    "trainer.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.28it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.92it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.41it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.37it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.38it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.93it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.69it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.88it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  3.71it/s]\n",
      "100%|██████████| 4/4 [00:01<00:00,  2.34it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.06it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.43it/s]\n"
     ]
    }
   ],
   "source": [
    "b_all = []\n",
    "label_all = []\n",
    "trainer.eval()\n",
    "for idx, (i,u,label) in enumerate(test_dataloader):\n",
    "    i, u = i.to(\"cuda\"), u.to(\"cuda\")\n",
    "    x = trainer.model.simulate(i,u)\n",
    "    a,b = trainer.barrier(x)\n",
    "    value = torch.einsum(\"btc,btc->bt\",a,u) + b\n",
    "    b_all.append((value < 0).cpu())\n",
    "    label_all.append(label.squeeze(-1))\n",
    "import numpy as np\n",
    "bs = torch.cat(b_all)\n",
    "labels = torch.cat(label_all)\n",
    "results = torch.cat([bs,labels.unsqueeze(-1)],dim=-1).detach().numpy()\n",
    "np.savetxt(\"./results_hyperplane_clip.txt\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4227814569536424 0.17333333333333334\n"
     ]
    }
   ],
   "source": [
    "results = np.loadtxt(\"./results_hyperplane_clip.txt\")\n",
    "regular = results[results[:,-1] == 0,:-1]\n",
    "collision = results[results[:,-1] == 1,:-1]\n",
    "acc_regular = (regular <= 0).mean()\n",
    "acc_collision = (collision > 0).mean()\n",
    "print(acc_regular,acc_collision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

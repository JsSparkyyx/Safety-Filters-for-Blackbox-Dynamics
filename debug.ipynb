{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.Safe2Unsafe import DeepAccidentDataset\n",
    "from method.ResNet import InDCBFTrainer, InDCBFController, Barrier\n",
    "import torch\n",
    "import time\n",
    "import pytorch_lightning as pl\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DeepAccidentDataset(train_batch_size=32,val_batch_size=32,num_workers=16)\n",
    "data.setup()\n",
    "train_dataloader = data.train_dataloader()\n",
    "test_dataloader = data.val_dataloader()\n",
    "for idx, (i,u,label) in enumerate(test_dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self,latent_dim,n_control=2,model=\"stabilityai/sd-vae-ft-mse\",hidden_dim=4*28*28,num_cam=6,freeze_ViT=True):\n",
    "        super(VAE, self).__init__()\n",
    "        from diffusers.models import AutoencoderKL\n",
    "        self.vae = AutoencoderKL.from_pretrained(model)\n",
    "        if freeze_ViT:\n",
    "            for n,p in self.vae.named_parameters():\n",
    "                p.requires_grad = False\n",
    "        self.proj = torch.nn.Linear(hidden_dim,latent_dim)\n",
    "        self.rec = torch.nn.Linear(latent_dim,hidden_dim)\n",
    "        self.fusion = torch.nn.Linear(2*latent_dim+n_control,latent_dim)\n",
    "        self.num_cam = num_cam\n",
    "    \n",
    "    def forward(self,imgs,x_p,u_p):\n",
    "        B,N,C,H,W = imgs.shape\n",
    "        rep = self.vae.encode(imgs.reshape(-1,C,H,W))['latent_dist'].mode().reshape(B,N,-1)\n",
    "        rep = self.proj(rep)\n",
    "        rep = torch.cat([rep,x_p,u_p.unsqueeze(1).expand(-1,N,-1)],dim=-1)\n",
    "        return self.fusion(rep)\n",
    "\n",
    "    def encode(self,imgs,x_p,u_p):\n",
    "        return self.forward(imgs,x_p,u_p)\n",
    "\n",
    "    def decode(self,x,trajectory=True):\n",
    "        if trajectory:\n",
    "            B,T,N,H = x.shape\n",
    "            rep = self.rec(x)\n",
    "            rep = rep.reshape(B*N*T,4,28,28)\n",
    "            return self.vae.decoder(rep).reshape(B,T,N,3,224,224)\n",
    "        else:\n",
    "            B,N,H = x.shape\n",
    "            rep = self.rec(x)\n",
    "            rep = rep.reshape(B*N,4,28,28)\n",
    "            return self.vae.decoder(rep).reshape(B,N,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(16,model=\"stabilityai/sd-vae-ft-mse\").cuda()\n",
    "x_init = torch.zeros(i.shape[0],6,16).cuda()\n",
    "u_init = torch.cat([u[:,0,:].unsqueeze(1),u],dim=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vae(i[:2,0,:].cuda(),x_init[:2],u_init[:2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = vae.decode(x,trajectory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from method.utils import build_mlp, NeuralODE\n",
    "latent_dim = 16\n",
    "h_dim = 256\n",
    "n_control = 2\n",
    "ode = NeuralODE([latent_dim,h_dim,h_dim,latent_dim],\n",
    "                             [latent_dim,h_dim,h_dim,latent_dim*n_control]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint\n",
    "def odefunc(t,state):\n",
    "    f, g = ode(state)\n",
    "    gu = torch.einsum(\"bnha,bna->bnh\",g.view(g.shape[0],g.shape[1],-1,n_control),u_init[:2,0].unsqueeze(1).expand(-1,6,-1))\n",
    "    return f + gu\n",
    "timesteps = torch.Tensor([0,0.1]).cuda()\n",
    "x_tide = odeint(odefunc,x,timesteps,rtol=5e-6)[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Barrier(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_control,\n",
    "                 latent_dim,\n",
    "                 num_cam = 6,\n",
    "                 h_dim = 64,\n",
    "                 eps_safe = 1,\n",
    "                 eps_unsafe = 1,\n",
    "                 eps_ascent = 1,\n",
    "                 eps_descent = 1,\n",
    "                 w_safe=1,\n",
    "                 w_unsafe=1,\n",
    "                 w_grad=1,\n",
    "                 w_non_zero=1,\n",
    "                 w_lambda=1,\n",
    "                 with_gradient=False,\n",
    "                 with_nonzero=False,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        super(Barrier, self).__init__()\n",
    "        modules = []\n",
    "        # hidden_dims = [latent_dim,h_dim,h_dim,h_dim,1]\n",
    "        hidden_dims = [latent_dim,h_dim,h_dim,1]\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(torch.nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            if not i == len(hidden_dims)-2:\n",
    "                modules.append(torch.nn.ReLU())\n",
    "        modules.append(torch.nn.Tanh())\n",
    "        self.attention = torch.nn.parameter.Parameter(torch.rand((num_cam,latent_dim)))\n",
    "        self.cbf = torch.nn.Sequential(*modules)\n",
    "        self.n_control = n_control\n",
    "        self.eps_safe = eps_safe\n",
    "        self.eps_unsafe = eps_unsafe\n",
    "        self.eps_ascent = eps_ascent\n",
    "        self.eps_descent = eps_descent\n",
    "        self.w_safe = w_safe\n",
    "        self.w_unsafe = w_unsafe\n",
    "        self.w_grad = w_grad\n",
    "        self.w_non_zero = w_non_zero\n",
    "        self.w_lambda = w_lambda\n",
    "        self.with_gradient = with_gradient\n",
    "        self.with_nonzero = with_nonzero\n",
    "\n",
    "    def forward(self,x):\n",
    "        B,T,N,H = x.shape\n",
    "        weight = torch.einsum(\"btnh,btnh->btn\",self.attention.expand(B,T,-1,-1),x)\n",
    "        x = torch.einsum(\"btn,btnh->btnh\",weight,x).sum(2)\n",
    "        return self.cbf(x)\n",
    "\n",
    "    def loss_function(self,x,label,u,ode):\n",
    "        # x = x.detach()\n",
    "        label = label.squeeze(dim=-1)\n",
    "        x_safe = x[label == 0]\n",
    "        x_unsafe = x[label == 1]\n",
    "        b_safe = self.forward(x_safe)\n",
    "        b_unsafe = self.forward(x_unsafe)\n",
    "        eps_safe = self.eps_safe*torch.ones_like(b_safe)\n",
    "        eps_unsafe = self.eps_unsafe*torch.ones_like(b_unsafe)\n",
    "        loss_1 = F.relu(eps_safe-b_safe).sum(dim=-1).mean()\n",
    "        loss_2 = F.relu(eps_unsafe+b_unsafe).sum(dim=-1).mean()\n",
    "        output = {\"loss_safe\":self.w_safe*loss_1,\"loss_unsafe\":self.w_unsafe*loss_2,\"b_safe\":b_safe.mean(),\"b_unsafe\":b_unsafe.mean()}\n",
    "        x_g = x_safe.clone().detach()\n",
    "        x_g.requires_grad = True\n",
    "        b = self.forward(x_g)\n",
    "        d_b_safe = torch.autograd.grad(b.mean(),x_g,retain_graph=True)[0]\n",
    "        with torch.no_grad():\n",
    "            f, g = ode(x_g)\n",
    "        gu = torch.einsum('btha,bta->bth',g.view(g.shape[0],g.shape[1],f.shape[-1],self.n_control),u[label == 0])\n",
    "        ascent_value = torch.einsum('bth,bth->bt', d_b_safe, (f + gu))\n",
    "        loss_3 = F.relu(self.eps_ascent - ascent_value.unsqueeze(-1) - b_safe).sum(dim=-1).mean()\n",
    "        output['loss_grad_ascent'] = self.w_grad*loss_3\n",
    "        output['b_grad_ascent'] = (ascent_value.unsqueeze(-1) + b_safe).mean()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barrier = Barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbdd3d0bef34cd6ace22da521ec896d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daa816d9c1748f183fcac086c406642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c427a0c29d9943699b43227a637d2529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf2127f97c541b89de8af676fc79b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32c0e6fa76847ec921f502562149e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import CLIPVisionModel, AutoTokenizer, CLIPTextModel\n",
    "\n",
    "model = \"openai/clip-vit-base-patch16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "image_encoder = CLIPVisionModel.from_pretrained(model)\n",
    "text_encoder = CLIPTextModel.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/root/DeepAccident/data/DeepAccident_data/type1_subtype1_accident/ego_vehicle/Camera_Front/Town01_type001_subtype0001_scenario00008\"\n",
    "img_name = \"{}_{:03d}.jpg\".format(\"Town01_type001_subtype0001_scenario00008\",1)\n",
    "img_path = os.path.join(path,img_name)\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "img = read_image(img_path)/255\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),     \n",
    "        ])\n",
    "sample = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([\"collision\"], padding=True, return_tensors=\"pt\")\n",
    "text_outputs = text_encoder(**inputs)['last_hidden_state'].mean(1)\n",
    "vision_outputs = image_encoder(pixel_values=sample.unsqueeze(0))['last_hidden_state'].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "img_name = \"{}_{:03d}.jpg\".format(\"Town01_type001_subtype0001_scenario00008\",1)\n",
    "img_path = os.path.join(path,img_name)\n",
    "image1 = Image.open(img_path)\n",
    "img_name = \"{}_{:03d}.jpg\".format(\"Town01_type001_subtype0001_scenario00008\",58)\n",
    "img_path = os.path.join(path,img_name)\n",
    "image2 = Image.open(img_path)\n",
    "img_name = \"{}_{:03d}.jpg\".format(\"Town01_type001_subtype0001_scenario00008\",60)\n",
    "img_path = os.path.join(path,img_name)\n",
    "image3 = Image.open(img_path)\n",
    "img_name = \"{}_{:03d}.jpg\".format(\"Town01_type001_subtype0001_scenario00008\",66)\n",
    "img_path = os.path.join(path,img_name)\n",
    "image4 = Image.open(img_path)\n",
    "\n",
    "inputs = processor(text=[\"no collision and accident\",\"collision\",\"collision imminent\"], images=[image1,image2,image3,image4], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40ca3c9ac064f3fb3ac27403449be5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3bdab16e3a4757a599089d2b1c377a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9767a0cdc10140778a579914c5fc1649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49597b99de394d47929424a3977304bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c397ae7fc2b74e7ba77a26a59370d786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/448M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/vit-mae-base')\n",
    "model = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-base')\n",
    "path = \"/root/DeepAccident/data/DeepAccident_data/type1_subtype1_accident/ego_vehicle/Camera_Front/Town01_type001_subtype0001_scenario00008\"\n",
    "img_name = \"{}_{:03d}.jpg\".format(\"Town01_type001_subtype0001_scenario00008\",1)\n",
    "img_path = os.path.join(path,img_name)\n",
    "image = Image.open(img_path)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.vit(inputs['pixel_values'])\n",
    "latent = outputs.last_hidden_state\n",
    "ids_restore = outputs.ids_restore\n",
    "mask = outputs.mask\n",
    "\n",
    "decoder_outputs = model.decoder(latent, ids_restore)\n",
    "i = model.unpatchify(decoder_outputs['logits'])\n",
    "# model.decoder(outputs.last_hidden_state,outputs.ids_restore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = model.vit(inputs['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(i.squeeze(0).detach().numpy())\n",
    "# plt.show()\n",
    "from torchvision.utils import save_image\n",
    "save_image(i.data[0],\"./test.png\")\n",
    "save_image(inputs['pixel_values'].data[0],\"./test1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
